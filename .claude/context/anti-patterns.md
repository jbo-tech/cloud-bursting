# Anti-patterns

Errors encountered and how to avoid them. Added via `/retro`.

<!-- Format:
### [Short title]
**Problem**: What went wrong
**Cause**: Why it happened
**Solution**: How to fix/avoid
**Date**: YYYY-MM-DD
-->

### Missing sqlite3 on cloud instance

**Problem**: All DB-based monitoring returns 0, causing false "already complete" detection and 60min timeouts with no progress tracking.
**Cause**: `sqlite3` not installed in `setup_instance.sh`. Commands fail silently (`check=False`) returning empty strings converted to 0.
**Solution**: Add `sqlite3` to apt-get install in cloud-init: `apt-get install -y ... sqlite3`
**Date**: 2026-01-20

### Missing timestamps in monitoring output

**Problem**: Monitoring logs without timestamps make it impossible to correlate events or measure durations.
**Cause**: Using simple `print(f"Activit√©: {n}")` without time context.
**Solution**: Always include `[HH:MM:SS]` prefix: `print(f"[{time.strftime('%H:%M:%S')}] üìã {activity_name} ({progress}%)")`
**Date**: inferred from codebase

### Generic "scanner actif" messages

**Problem**: Same message displayed during both scan and analyze phases, making it hard to track progress.
**Cause**: Not differentiating between scan and analyze phases in `wait_section_idle()`.
**Solution**: Use `phase='scan'` or `phase='analyze'` parameter to customize icons and messages per phase.
**Date**: inferred from codebase

### Fixed timeout for all section types

**Problem**: Photos section terminates prematurely because thumbnail generation is much slower than video/music.
**Cause**: Using 1h timeout for all section types.
**Solution**: Pass `section_type` to `wait_section_idle()`. Photos automatically get 4h timeout.
**Date**: inferred from codebase

### Parallel section processing

**Problem**: Launching all scans then waiting globally causes resource contention and unpredictable behavior.
**Cause**: `for s in sections: trigger_scan(s)` then `wait_all()` pattern.
**Solution**: Sequential processing: `for s in sections: trigger_scan(s); wait_idle(s)`. Disable background tasks before isolated processing.
**Date**: inferred from codebase

### Forgetting photos in statistics

**Problem**: Statistics summaries missing photo counts.
**Cause**: Photos have `metadata_type=13` which is easy to forget.
**Solution**: Always include `'photos': 0` in stats dictionaries. Use DB queries with correct metadata_type mapping.
**Date**: inferred from codebase

### API-only counting

**Problem**: Plex API returns stale or incomplete data during heavy scanning.
**Cause**: Relying solely on Plex API for progress tracking.
**Solution**: Prefer direct SQLite queries to `com.plexapp.plugins.library.db` for accurate counts.
**Date**: inferred from codebase

### Using undefined function

**Problem**: Script crashes with `NameError` because function doesn't exist.
**Cause**: Calling `trigger_analysis_all()` which was never implemented (copy-paste error).
**Solution**: Verify function existence in source module before using. Correct function: `trigger_deep_analysis()`.
**Date**: inferred from codebase

### Missing imports after refactoring

**Problem**: Script crashes because function is used but not imported.
**Cause**: Refactoring moves code to `common/` but forgets to update imports.
**Solution**: After any refactoring, verify all imports. Common culprits: `find_latest_db_archive`, `read_state_file`, `trigger_scan_all`.
**Date**: inferred from codebase

### Using undefined CLI argument

**Problem**: Script crashes with `AttributeError: args has no attribute 'skip_scan'`.
**Cause**: Using `args.skip_scan` without `parser.add_argument('--skip-scan')`.
**Solution**: Verify every `args.xxx` has a corresponding `add_argument('--xxx')` definition.
**Date**: inferred from codebase

### Redundant CLI arguments

**Problem**: Confusing UX with multiple arguments doing similar things (`--skip-analysis` + `--quick-test`).
**Cause**: Organic growth without harmonization.
**Solution**: One argument per behavior. Use `--quick-test` consistently across scripts.
**Date**: inferred from codebase

### Wrong stall timeout calculation

**Problem**: Comments say "10 min stall timeout" but actual timeout is different.
**Cause**: Forgetting formula: `stall_threshold √ó check_interval = actual_time`.
**Solution**: Always verify: `local_quick: 5 √ó 30s = 2.5min`, `local_delta: 10 √ó 60s = 10min`, `cloud_intensive: 30 √ó 120s = 60min`.
**Date**: inferred from codebase

### Wildcard imports

**Problem**: Unclear which functions are available, potential name collisions.
**Cause**: Using `from module import *` for convenience.
**Solution**: Always use explicit imports: `from common.plex_scan import trigger_section_scan, wait_section_idle`.
**Date**: inferred from codebase

### Imports inside function body

**Problem**: Hidden dependencies, harder to track what module needs.
**Cause**: Lazy loading without clear justification.
**Solution**: Imports at top of file. Exception: lazy loading for expensive optional dependencies (document why).
**Date**: inferred from codebase

### Duplicating code between scripts

**Problem**: Bug fixes need to be applied in multiple places, drift between implementations.
**Cause**: Copy-pasting instead of extracting to common module.
**Solution**: Centralize shared logic in `common/` modules. Scripts should only contain orchestration logic.
**Date**: inferred from codebase

### Single-shot token retrieval

**Problem**: Token Plex non r√©cup√©r√© car appel unique sans retry. Le token peut mettre plusieurs secondes √† appara√Ætre dans Preferences.xml apr√®s le claim.
**Cause**: `get_plex_token()` faisait un seul appel et abandonnait imm√©diatement si le token n'√©tait pas pr√©sent.
**Solution**: Impl√©menter retry avec timeout (120s par d√©faut, intervalle 10s). Le token appara√Æt g√©n√©ralement apr√®s quelques secondes.
**Date**: 2026-01-21

### Insufficient init failure diagnostics

**Problem**: `wait_plex_fully_ready()` timeout sans indication de pourquoi l'API `/identity` √©choue. Impossible de diagnostiquer.
**Cause**: Logging minimal ("Plex initialisation... (processus: N)") sans d√©tail sur l'√©tat de l'API.
**Solution**: Logger le code HTTP et la r√©ponse √† chaque it√©ration. En cas de timeout, capturer automatiquement les 20 derni√®res lignes des logs Docker.
**Date**: 2026-01-21

### Using --force with Sonic analysis triggers metadata refresh

**Problem**: Sonic analysis blocked for 2h (compteur 81,035 ‚Üí 81,035) malgr√© CPU 407%. Aucun fichier audio lu depuis S3.
**Cause**: `--force` dans `trigger_sonic_analysis()` d√©clenche un refresh metadata complet (fanart.tv, lastfm, paroles) AVANT l'analyse audio Chromaprint. Le CPU √©tait occup√© √† t√©l√©charger des images, pas √† analyser l'audio.
**Solution**: Retirer `--force` de l'analyse Sonic. S√©parer explicitement: (1) metadata refresh optionnel avec `--force-refresh`, (2) stabilisation (attente idle), (3) analyse Sonic sans --force.
**Date**: 2026-01-23

### Permission denied errors as symptom of dead rclone mount

**Problem**: Erreurs "Permission denied" lors de la cr√©ation de bundles metadata (24 erreurs), semblant indiquer un probl√®me de droits.
**Cause**: Le montage rclone FUSE s'est d√©connect√© (socket mort) mais reste mont√© en apparence. Le noyau Linux retourne des erreurs incoh√©rentes: "Socket not connected" pour les lectures, "Permission denied" pour les √©critures.
**Solution**: Ces erreurs sont un faux positif. Ne pas corriger les permissions - corriger la stabilit√© du montage rclone. Les erreurs dispara√Ætront une fois le montage fiabilis√©.
**Date**: 2026-01-24

### Rclone mount disconnecting after ~30 minutes

**Problem**: Test de nuit √©choue avec 1248 erreurs "Socket not connected". Le montage S3 devient inaccessible apr√®s ~30 minutes d'inactivit√©.
**Cause**: Configuration rclone par d√©faut insuffisante pour les connexions longue dur√©e: timeout 10m trop court, pas de retries automatiques, pas de reconnexion.
**Solution**: Augmenter les param√®tres de r√©silience: `--timeout 30m`, `--contimeout 300s`, `--retries 10`, `--retries-sleep 30s`, `--low-level-retries 10`. Ajouter `--stats 5m` pour monitoring.
**Date**: 2026-01-24

### MountHealthMonitor started too late

**Problem**: Le montage rclone devient instable pendant le prompt PLEX_CLAIM (d√©lai utilisateur). Plex d√©marre avec un montage d√©faillant.
**Cause**: MountHealthMonitor d√©marr√© APR√àS le prompt utilisateur, pas avant. Pendant que l'utilisateur entre son claim token (potentiellement plusieurs minutes), le montage n'est pas surveill√©.
**Solution**: D√©marrer MountHealthMonitor AVANT le prompt PLEX_CLAIM. Le monitor surveille le montage pendant le d√©lai utilisateur et peut faire un remontage automatique si n√©cessaire.
**Date**: 2026-01-29

### enable_plex_analysis_via_api() called before scan

**Problem**: wait_section_idle() timeout apr√®s 144 minutes. Le scan CLI termine rapidement mais l'attente est bloqu√©e.
**Cause**: enable_plex_analysis_via_api() appel√©e en phase 4 (avant scan) d√©clenche le Butler DeepMediaAnalysis. Les processus `Plex Media Scanner --analyze-deeply` sont d√©tect√©s par pgrep comme "scanner running", bloquant wait_section_idle().
**Solution**: Ne PAS appeler enable_plex_analysis_via_api() avant le scan. Les analyses Sonic sont correctement d√©clench√©es par enable_music_analysis_only() en phase 6.3 (apr√®s le scan). Pour les autres sections, trigger_section_analyze() d√©clenche l'analyse par section via API.
**Date**: 2026-01-29

### Confusing Plex Scanner flags behavior

**Problem**: Flag `--force` fait plus que forcer l'analyse - il d√©clenche aussi un refresh de toutes les m√©tadonn√©es.
**Cause**: Documentation Plex insuffisante sur les effets de bord de `--force`.
**Solution**: Documenter les flags Plex Scanner: `--force` = refresh metadata + action demand√©e. Pour analyse seule, ne pas utiliser `--force`. V√©rifier dans les logs: "Updating Metadata" = refresh, "Fingerprinting"/"Sonic" = analyse audio.
**Date**: 2026-01-23

---

## Reference: Plex Metadata Types

```python
PLEX_METADATA_TYPES = {
    1: 'movie',      # Film
    2: 'show',       # S√©rie TV
    3: 'season',     # Saison
    4: 'episode',    # √âpisode
    8: 'artist',     # Artiste musical
    9: 'album',      # Album musical
    10: 'track',     # Piste audio
    13: 'photo',     # Photo
    14: 'photoalbum' # Album photo
}

PLEX_SECTION_TYPES = {
    'artist': 'Musique',
    'movie': 'Films',
    'show': 'S√©ries TV',
    'photo': 'Photos'
}
```

### Holding lock during long-running operations (deadlock)

**Problem**: Script bloqu√© ind√©finiment apr√®s entr√©e PLEX_CLAIM. Aucun message, aucune progression pendant 4+ heures.
**Cause**: `_perform_health_check()` dans MountHealthMonitor d√©tient `self._lock` pendant 30+ secondes (v√©rification montage + remontage √©ventuel). `clear_pending_input()` appel√© depuis le thread principal tente d'acqu√©rir le m√™me lock et reste bloqu√©.
**Solution**: Ne jamais d√©tenir un lock pendant des op√©rations I/O longues. Mieux: √©viter le pattern o√π le thread principal interagit avec le thread monitor. Solution adopt√©e: input AVANT d√©marrage du monitor.
**Date**: 2026-01-30

### Monitor starting before user input (UX + timing issues)

**Problem**: Messages du MountHealthMonitor s'affichent pendant que l'utilisateur attend le prompt PLEX_CLAIM, cr√©ant confusion.
**Cause**: Le monitor d√©marre avant l'input(), le premier health check s'ex√©cute imm√©diatement, et les messages (stdout) apparaissent avant ou apr√®s le prompt, masquant l'attente d'input.
**Solution**: Toujours demander les inputs utilisateur AVANT de d√©marrer les threads de monitoring. L'input interactif doit √™tre isol√© de tout background processing.
**Date**: 2026-01-30

## Reference: Timeout Formulas

```python
# Temps r√©el avant arr√™t sur stall
temps_stall = stall_threshold √ó check_interval

# Profils actuels:
# local_quick    : 5 √ó 30s   = 2.5 min
# local_delta    : 10 √ó 60s  = 10 min
# cloud_intensive: 30 √ó 120s = 60 min (1h)
```

### Uninitialized variable in conditional block

**Problem**: `NameError: name 'stats_after_scan' is not defined` si la phase Music est skipp√©e via `--section Movies`.
**Cause**: `stats_after_scan` √©tait assign√©e uniquement dans le bloc `if should_process_music:`. Si la condition est False, la variable n'existe pas mais est utilis√©e plus tard.
**Solution**: Initialiser la variable AVANT le bloc conditionnel: `stats_after_scan = stats_before`. Toujours initialiser les variables qui seront utilis√©es hors du bloc o√π elles sont potentiellement assign√©es.
**Date**: 2026-01-31

### CLI argument name mismatch (args.X vs --Y)

**Problem**: `AttributeError: 'Namespace' object has no attribute 'only'` - le script crashe √† l'acc√®s d'un attribut inexistant.
**Cause**: L'argument CLI est d√©fini comme `--section` (stock√© dans `args.section`) mais le code utilise `args.only` (copie d'un autre script ou refactoring incomplet).
**Solution**: V√©rifier que chaque `args.xxx` correspond √† un `add_argument('--xxx')`. Apr√®s renommage d'arguments, rechercher toutes les occurrences de l'ancien nom dans le fichier.
**Date**: 2026-01-31

### Deadlock in cleanup method due to lock held by worker thread

**Problem**: Script bloqu√© apr√®s "‚úÖ DELTA SYNC TERMIN√â" - la m√©thode `stop()` de MountHealthMonitor ne retourne jamais.
**Cause**: Dans `stop()`, appel √† `_print_stats()` qui tente d'acqu√©rir `self._lock`. Ce lock est d√©j√† d√©tenu par le thread de health check (`_run()` ‚Üí `_perform_health_check()`). KeyboardInterrupt peut arriver pendant que le thread d√©tient le lock.
**Solution**: Acqu√©rir le lock avec timeout dans les m√©thodes de cleanup: `if self._lock.acquire(timeout=2): ... else: print("lock timeout")`. Ne jamais bloquer ind√©finiment dans finally/cleanup.
**Date**: 2026-02-04

### Silent database corruption causing Plex restart loop

**Problem**: Plex d√©marre puis crashe en boucle avec "database disk image is malformed" dans les logs. L'erreur n'est visible que dans les logs Docker, pas dans le script.
**Cause**: Archive DB corrompue inject√©e sans validation. L'extraction r√©ussit mais la DB est inutilisable. Pas de v√©rification d'int√©grit√© avant d√©marrage Plex.
**Solution**: Ex√©cuter `PRAGMA integrity_check;` apr√®s extraction de la DB. V√©rifier que le r√©sultat est exactement "ok" (lowercase). √âchouer imm√©diatement si la DB est corrompue, avant de d√©marrer Plex.
**Date**: 2026-02-04

### PRAGMA integrity_check fails on Plex FTS tables

**Problem**: `PRAGMA integrity_check;` √©choue avec "unknown tokenizer: collating" sur une DB Plex valide.
**Cause**: Plex utilise des tables FTS (Full-Text Search) avec tokenizers personnalis√©s non support√©s par le sqlite3 syst√®me. Le PRAGMA essaie de v√©rifier ces tables et √©choue.
**Solution**: Remplacer `PRAGMA integrity_check` par une requ√™te simple sur une table basique: `SELECT COUNT(*) FROM library_sections;`. Valide que la DB est lisible sans toucher aux tables FTS.
**Date**: 2026-02-05

### Audit false positive: SQL injection in local config files

**Problem**: Audit signale une injection SQL sur des chemins ins√©r√©s dans des requ√™tes sqlite3.
**Cause**: Le fichier `path_mappings.json` est contr√¥l√© par l'utilisateur local, pas expos√© √† des inputs externes.
**R√©alit√©**: FAUX POSITIF. Un attaquant ayant acc√®s √† ce fichier aurait d√©j√† un acc√®s complet au syst√®me. Le risque r√©el est proche de z√©ro dans ce contexte.
**Note**: Seul cas valide: si les chemins contiennent des apostrophes (`O'Brien`), √©chapper avec `s.replace("'", "''")`.
**Date**: 2026-02-05

### Audit false positive: imports inside functions

**Problem**: Audit critique les `import json`, `import shutil` √† l'int√©rieur des fonctions au lieu du haut du fichier.
**Cause**: Pattern de lazy import pour √©viter de charger des modules inutilis√©s.
**R√©alit√©**: FAUX POSITIF. Python cache les imports, pas d'impact performance. Pattern acceptable et coh√©rent avec le reste du projet (ex: `import traceback` dans les blocs except).
**Date**: 2026-02-05

### Forgetting to update all scripts after adding a feature

**Problem**: Feature ajout√©e dans `test_delta_sync.py` mais pas dans `automate_delta_sync.py`. Le test local fonctionne mais la production cloud √©choue.
**Cause**: Les scripts local et cloud partagent les m√™mes modules `common/` mais ont leur propre orchestration. Facile d'oublier de propager les changements.
**Solution**: Apr√®s ajout d'une feature touchant le workflow, toujours v√©rifier les 4 scripts: `test_scan_local.py`, `test_delta_sync.py`, `automate_scan.py`, `automate_delta_sync.py`.
**Date**: 2026-02-05

### os.path.exists(None) crashes with cryptic error

**Problem**: `collect_plex_logs()` crashe avec "stat: path should be string, bytes, os.PathLike or integer, not NoneType".
**Cause**: `terminal_log=None` pass√© par d√©faut, puis `os.path.exists(terminal_log)` appel√© sans v√©rifier que la variable n'est pas None.
**Solution**: Toujours v√©rifier `if variable and os.path.exists(variable)` pour les chemins optionnels. Pattern: `if terminal_log and os.path.exists(terminal_log)`.
**Date**: 2026-02-05

### Stats reading during rclone timeout gives wrong values

**Problem**: Le r√©capitulatif affiche "728 √©pisodes (+-210)" alors que la DB contient r√©ellement 938 √©pisodes.
**Cause**: La lecture des stats via sqlite3 a √©t√© effectu√©e pendant un timeout rclone (montage bloqu√©). La requ√™te a retourn√© une valeur partielle ou incorrecte.
**Solution**: V√©rifier l'√©tat du montage rclone avant de lire les stats. Les stats finales doivent √™tre lues apr√®s stabilisation du montage, pas pendant une p√©riode de timeout/remontage.
**Date**: 2026-02-05

### Diagnostic Sonic displayed even when Music not selected

**Problem**: Le diagnostic post-mortem affiche "üéπ DIAGNOSTIC SONIC" m√™me quand `--section TV Shows` (pas de musique).
**Cause**: Le bloc diagnostic Sonic n'√©tait pas conditionn√© par `should_process_music`.
**Solution**: Conditionner avec `if should_process_music:` et initialiser `should_process_music = True` en dehors du try/except pour qu'elle soit accessible dans finally.
**Date**: 2026-02-05

### Holding lock during long I/O operations (MountMonitor v2)

**Problem**: `stop()` affiche "Stats indisponibles (lock timeout)" au lieu des statistiques. Le script met 7+ secondes √† s'arr√™ter.
**Cause**: `_perform_health_check()` d√©tenait `self._lock` pendant toute la dur√©e du health check (30s) + remontage potentiel. `stop()` ne pouvait pas acqu√©rir le lock (join 5s + acquire 2s < health check 30s).
**Solution**: S√©parer lock et I/O : les op√©rations longues (verify_rclone, remount) s'ex√©cutent SANS lock. Le lock n'est acquis que pour les mises √† jour d'√©tat (microsecondes). Utiliser `threading.Event` pour interrompre le sleep imm√©diatement.
**Date**: 2026-02-05

### Residential internet NAT saturation during parallel S3 access

**Problem**: 2375 erreurs rclone "connection reset by peer" lors de l'analyse de 28k photos via S3 Scaleway. Analyse bloqu√©e 4h avec compteur oscillant (28168‚Üî28326).
**Cause**: Les box r√©sidentielles (Free/Orange) limitent les sessions NAT concurrentes (~4096). L'analyse parall√®le de milliers de fichiers sature cette limite. Le streaming (1 fichier s√©quentiel) fonctionne car il n'ouvre qu'une connexion.
**Solution**: Les workloads d'analyse massive doivent s'ex√©cuter en cloud (m√™me datacenter que S3). Les tests locaux ne sont viables que pour les petites biblioth√®ques (Movies: ~300 items). Ne pas confondre "le streaming marche" avec "l'analyse marchera".
**Date**: 2026-02-05

### Scan on degraded rclone mount deletes DB entries

**Problem**: Plex scanner supprime 221/224 films de la DB. Le scan progresse (0%‚Üí99%) mais ne trouve aucun fichier. R√©sultat: `Films: 94 (+-221)`.
**Cause**: Le montage rclone est en √©tat d√©grad√© (I/O bloqu√©). Le dir-cache (72h) permet de lister les r√©pertoires, mais les fichiers sont inaccessibles. Plex interpr√®te "r√©pertoire listable, fichiers inaccessibles" comme "fichiers supprim√©s" et purge la DB.
**Solution**: Appeler `ensure_mount_healthy()` avant chaque `trigger_section_scan()`. Si le montage est cass√©, annuler le scan. Ne JAMAIS scanner sur un montage d√©grad√© ‚Äî les d√©g√¢ts sont irr√©versibles.
**Risque r√©siduel**: Le montage peut tomber PENDANT un scan (fen√™tre de 60s entre les checks du MountMonitor). Pas de solution simple sans sur-ing√©nierie. Accepter le risque.
**Date**: 2026-02-09

### MountMonitor remount survives stop() and runs during cleanup

**Problem**: Apr√®s `mount_monitor.stop()`, des messages "üîÑ Tentative de remontage 1/3..." apparaissent pendant le cleanup (apr√®s suppression des dossiers de test).
**Cause**: `remount_s3_if_needed()` prenait ~3-4 min (3 retries avec cooldowns). Le `join(timeout=35s)` expirait, le thread daemon continuait en arri√®re-plan pendant le cleanup.
**Solution**: Passer un `stop_event` (threading.Event) √† `remount_s3_if_needed()`. Remplacer `time.sleep()` par `stop_event.wait(timeout=)` et v√©rifier `stop_event.is_set()` entre chaque retry. Le thread s'arr√™te en quelques secondes au lieu de minutes.
**Date**: 2026-02-09

### Docker image pull during scan phase wastes 30 minutes

**Problem**: 30 min d'√©cart entre `docker run` et le d√©marrage effectif de Plex. Le MountMonitor tourne pour rien, le claim token peut expirer (4 min de validit√©).
**Cause**: L'image `plexinc/pms-docker:latest` n'√©tait pas en cache. `docker run` t√©l√©charge l'image avant de d√©marrer le conteneur.
**Solution**: Ajouter `docker pull` en Phase 1 (pr√©paration), avant le montage S3 et le MountMonitor. D√©j√† fait dans `setup_instance.sh` pour le cloud, ajout√© dans les scripts locaux.
**Date**: 2026-02-09

### Plex library with multiple locations pointing to different mount paths

**Problem**: Biblioth√®que Photos a 2 locations (`/Media/Photo` + `/Photo`), mais le Docker ne monte que `/Media`. Toutes les photos sous `/Photo` √©chouent avec "FreeImage_Load: failed to open file".
**Cause**: Configuration Plex historique avec un chemin local (`/Photo`) en plus du chemin S3 (`/Media/Photo`). Le chemin local n'est pas mont√© dans le conteneur cloud.
**Solution**: Ajouter un mapping dans `path_mappings.json` pour consolider les chemins vers S3 (`/Photo` ‚Üí `/Media/Photo`). V√©rifier syst√©matiquement que TOUS les chemins de la DB sont accessibles via le montage Docker.
**Date**: 2026-02-05

### MountMonitor with aggressive timeout on slow networks causes silent scan failure

**Problem**: Scan de Movies retourne +0 delta alors que des fichiers ont √©t√© ajout√©s. Le scan semble r√©ussir (220/221 analys√©s) mais ne d√©tecte aucun nouveau fichier. Dans un second test, la machine g√®le compl√®tement.
**Cause**: Le healthcheck du MountMonitor utilise un timeout de 30s. Sur connexion r√©sidentielle (latence variable, NAT), les lectures S3 d√©passent r√©guli√®rement 30s ‚Üí faux positif ‚Üí remontage automatique ‚Üí dir-cache rclone purg√© ‚Üí Plex ne voit que les fichiers d√©j√† en DB, pas les nouveaux. Le remontage pendant des I/O FUSE actives peut aussi geler le syst√®me.
**Solution**: Ne pas utiliser MountMonitor en local. Les param√®tres rclone de r√©silience (`--timeout 120m`, `--retries 20`) suffisent. R√©server MountMonitor pour le cloud (latence S3 <1ms, timeout 30s = vrai probl√®me).
**Date**: 2026-02-11

### MountMonitor running during export/cleanup phases

**Problem**: Messages "Tentative de remontage" pendant l'export de la DB et le diagnostic post-mortem. Le monitor remonte inutilement alors que le montage S3 n'est plus n√©cessaire.
**Cause**: `mount_monitor.stop()` dans le `finally` block, donc le monitor tourne pendant toute la phase Export qui ne lit que le disque local.
**Solution**: Stopper le monitor AVANT la phase Export (`mount_monitor.stop(); mount_monitor = None`). Garder un filet de s√©curit√© dans finally pour le cas d'exception avant l'export.
**Date**: 2026-02-11

### Scanner sees directories but not files inside (rclone FUSE)

**Problem**: Plex scanner trouve les dossiers (`Processing directory /Media/Movies/Dune (2021)`) mais pas les fichiers dedans (`File 'Dune (2021) Bluray-720p.mp4' didn't exist`). R√©sultat: 0 ajout√©, 221 supprim√© en 2 secondes.
**Cause**: Soit les fichiers en S3 ont des noms diff√©rents de ceux enregistr√©s en DB (archive DB de d√©cembre 2025, fichiers potentiellement renomm√©s depuis), soit le montage rclone FUSE ne liste pas correctement le contenu des sous-r√©pertoires. Le rclone stats montre `Listed 586490` mais `Transferred: 0 B`.
**Solution**: Avant de lancer un delta sync, v√©rifier que les fichiers existent dans S3 avec les M√äMES noms que dans la DB. Commande: `rclone ls mega-s4:media-center/Movies/<dossier>/ --config ./rclone.conf`. Si les noms ont chang√©, un scan from scratch est n√©cessaire (pas un delta sync).
**Date**: 2026-02-13

### DB corruption during SQL remapping (intermittent)

**Problem**: `database disk image is malformed` pendant le UPDATE SQL de remapping des chemins. Plex crashe en boucle ensuite.
**Cause**: La DB Plex (15 GB, tables FTS) peut avoir une corruption latente non d√©tect√©e par `SELECT COUNT(*) FROM library_sections`. Le simple SELECT lit quelques pages, pas les tables FTS ni les index. Un UPDATE massif (30k+ lignes dans media_parts) expose la corruption.
**Solution**: `repair_plex_db()` dans `common/delta_sync.py` d√©tecte la corruption via `SELECT COUNT(*) FROM media_parts` et r√©pare via `.recover`. Appel√©e automatiquement avant le remapping.
**Date**: 2026-02-13 (r√©solu 2026-02-23)

### sqlite3 .dump fails on B-tree corruption (produces empty file)

**Problem**: `sqlite3 db '.dump' | sqlite3 repaired.db` produit un fichier de 0 octets sur une DB avec index B-tree corrompus.
**Cause**: `.dump` traverse les index et les donn√©es s√©quentiellement. Si un index corrompu bloque la lecture d'une table, le dump s'arr√™te avec "database disk image is malformed" et ne produit aucune sortie SQL pour cette table.
**Solution**: Utiliser `.recover` au lieu de `.dump`. `.recover` parcourt les pages raw de la DB et reconstruit les donn√©es ind√©pendamment des index. Les tables internes SQLite (sqlite_stat1, sqlite_sequence) sont perdues (7/82) mais Plex les recr√©e au d√©marrage. Toutes les tables de donn√©es (media_parts, metadata_items, etc.) sont r√©cup√©r√©es.
**Date**: 2026-02-23
