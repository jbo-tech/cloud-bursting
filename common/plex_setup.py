#!/usr/bin/env python3
"""
Gestion du cycle de vie Plex : montage, d√©marrage, configuration
"""
import time
import os
import re
from datetime import datetime
from .executor import execute_command, docker_exec, transfer_file_to_remote, execute_script
from .config import get_rclone_config_path, get_rclone_profile, get_rclone_remote_name

def apply_system_optimizations(ip):
    """
    Applique les optimisations syst√®me (sysctl, ulimits) pour l'environnement de production.
    Uniquement sur les instances distantes (pas en local).

    Args:
        ip: 'localhost' ou IP remote
    """
    if ip == 'localhost':
        print("‚è≠Ô∏è  Optimisations syst√®me d√©sactiv√©es en mode local")
        return

    print("‚öôÔ∏è  Application des optimisations syst√®me...")

    sysctl_script = """#!/bin/bash
# Optimisations sysctl pour Plex
cat << 'EOF' | sudo tee -a /etc/sysctl.conf
fs.file-max=500000
vm.swappiness=10
vm.vfs_cache_pressure=50
EOF

sudo sysctl -p

# Optimisations ulimits
cat << 'EOF' | sudo tee -a /etc/security/limits.conf
* soft nofile 100000
* hard nofile 500000
EOF

echo "‚úÖ Optimisations syst√®me appliqu√©es"
"""

    execute_script(ip, sysctl_script, '/tmp/apply_system_optimizations.sh')
    print("‚úÖ Optimisations syst√®me appliqu√©es")


def cleanup_plex_data(ip):
    """
    Supprime les donn√©es Plex pour repartir sur un √©tat propre.

    Args:
        ip: 'localhost' ou IP remote
    """
    print("üßπ Nettoyage des donn√©es Plex...")
    execute_command(ip, "rm -rf /opt/plex_data/*", check=False)
    execute_command(ip, "mkdir -p /opt/plex_data/{config,transcode}")
    execute_command(ip, "chmod -R 777 /opt/plex_data")


def setup_rclone_config(ip):
    """
    Copie le fichier rclone.conf vers /tmp.
    - Local: utilise ./tmp du projet pour ne pas √©craser ~/.config/rclone/
    - Remote: utilise /tmp de l'instance (√©ph√©m√®re)

    Args:
        ip: 'localhost' ou IP remote

    Returns:
        str: Chemin du fichier rclone.conf copi√©
    """
    rclone_config = get_rclone_config_path()

    if ip == 'localhost':
        # Utiliser ./tmp du projet pour ne pas interf√©rer avec la config syst√®me
        local_tmp = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tmp')
        print(f"üìã [LOCAL] Copie rclone.conf vers {local_tmp}/")
        execute_command(ip, f"mkdir -p {local_tmp}", check=False)
        execute_command(ip, f"cp {rclone_config} {local_tmp}/rclone.conf")
        print(f"‚úÖ rclone.conf copi√© vers {local_tmp}/rclone.conf")
        return f"{local_tmp}/rclone.conf"

    print("üì§ Copie de rclone.conf sur l'instance...")
    transfer_file_to_remote(rclone_config, ip, '/tmp/rclone.conf')
    print("‚úÖ rclone.conf copi√© vers /tmp/rclone.conf")
    return '/tmp/rclone.conf'


def mount_s3(ip, rclone_remote, profile='lite', mount_point='/mnt/s3-media', cache_dir=None, log_file=None, config_path=None):
    """
    Monte le bucket S3 via rclone avec une configuration optimis√©e.

    Args:
        ip: 'localhost' ou IP remote
        rclone_remote: Nom du bucket/chemin S3
        profile: Profil de configuration (lite, balanced, performance)
        mount_point: Point de montage
        cache_dir: R√©pertoire de cache rclone
        log_file: Fichier de logs rclone
        config_path: Chemin du fichier rclone.conf (si None, utilise ~/.config/rclone/rclone.conf par d√©faut)
    """
    print(f"üì¶ Montage S3 : {rclone_remote} ‚Üí {mount_point} (profil: {profile})")

    # R√©cup√©ration du nom du remote rclone
    remote_name = get_rclone_remote_name()

    # D√©tecter le chemin de config si non fourni
    if config_path is None:
        if ip == 'localhost':
            local_tmp = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tmp')
            config_path = f"{local_tmp}/rclone.conf"
        else:
            config_path = '/tmp/rclone.conf'

    print(f"üîß Utilisation de la config rclone : {config_path}")

    # Auto-d√©tection des chemins selon le contexte
    if cache_dir is None:
        if ip == 'localhost':
            cache_dir = os.path.expanduser('~/tmp/rclone-cache')
        else:
            cache_dir = '/mnt/rclone-cache'

    if log_file is None:
        if ip == 'localhost':
            log_file = os.path.expanduser('~/tmp/rclone.log')
        else:
            log_file = '/var/log/rclone.log'

    config = get_rclone_profile(profile)

    mount_script = f"""#!/bin/bash
set -e

echo "üîß Nettoyage pr√©alable..."
pkill -f "rclone mount" || true
sleep 2
fusermount3 -uz {mount_point} || true

echo "üîß Cr√©ation des r√©pertoires..."
mkdir -p {mount_point} {cache_dir}

echo "üîß V√©rification du remote rclone '{remote_name}'..."
if ! rclone --config {config_path} listremotes | grep -q "^{remote_name}:$"; then
    echo "‚ùå Remote '{remote_name}' non trouv√©"
    rclone --config {config_path} listremotes
    exit 1
fi

echo "üîß Test de connexion au remote..."
if ! rclone --config {config_path} lsd {remote_name}:{rclone_remote} --max-depth 1 2>&1 | head -10; then
    echo "‚ùå Impossible de lister le contenu du remote"
    exit 1
fi

echo "üîß V√©rification de /etc/fuse.conf..."
if ! grep -q "^user_allow_other" /etc/fuse.conf 2>/dev/null; then
    echo "‚ö†Ô∏è  ATTENTION: user_allow_other n'est pas activ√© dans /etc/fuse.conf"
    echo "   Cela peut causer des probl√®mes avec Docker + FUSE"
    echo "   Solution: sudo nano /etc/fuse.conf et d√©commenter 'user_allow_other'"
fi

echo "üîß Lancement du montage rclone..."
nohup rclone mount {remote_name}:{rclone_remote} {mount_point} \\
  --config {config_path} \\
  --cache-dir={cache_dir} \\
  --vfs-cache-mode full \\
  --vfs-cache-max-size {config['cache_size']} \\
  --vfs-cache-max-age 72h \\
  --vfs-read-ahead {config['buffer_size']} \\
  --vfs-read-chunk-size {config['read_chunk']} \\
  --vfs-read-chunk-size-limit 1G \\
  --buffer-size {config['buffer_size']} \\
  --transfers {config['transfers']} \\
  --checkers {config['checkers']} \\
  --timeout {config['timeout']} \\
  --contimeout {config['contimeout']} \\
  --low-level-retries {config['low_level_retries']} \\
  --retries {config['retries']} \\
  --retries-sleep {config['retries_sleep']} \\
  --dir-cache-time {config['dir_cache']} \\
  --attr-timeout {config['attr_timeout']} \\
  --poll-interval 1m \\
  --s3-no-head \\
  --no-checksum \\
  --allow-other \\
  --uid 1000 \\
  --gid 1000 \\
  --log-level INFO \\
  --log-file={log_file} \\
  --stats 5m \\
  --stats-log-level INFO \\
  --daemon </dev/null >/dev/null 2>&1 &

echo "‚è≥ Attente de stabilisation du montage (10s)..."
sleep 10

# V√©rifications multiples
echo "üîç V√©rification 1: mountpoint"
if ! mountpoint -q {mount_point}; then
    echo "‚ùå Le point de montage n'est pas actif"
    exit 1
fi
echo "‚úÖ mountpoint OK"

echo "üîç V√©rification 2: stat"
if ! stat {mount_point} > /dev/null 2>&1; then
    echo "‚ùå Impossible de faire stat sur le point de montage"
    exit 1
fi
echo "‚úÖ stat OK"

echo "üîç V√©rification 3: ls (test de lecture)"
if ! ls {mount_point} > /dev/null 2>&1; then
    echo "‚ùå Impossible de lire le contenu du point de montage"
    tail -20 {log_file}
    exit 1
fi
echo "‚úÖ ls OK"

echo "üîç V√©rification 4: Permissions"
ls -la {mount_point} | head -5

echo "‚úÖ Montage S3 complet et valid√©"
"""

    print(f"""
rclone mount {remote_name}:{rclone_remote} {mount_point} \\
    --config {config_path} \\
    --cache-dir={cache_dir} \\
    --vfs-cache-mode full \\
    --vfs-cache-max-size {config['cache_size']} \\
    --vfs-cache-max-age 72h \\
    --vfs-read-ahead {config['buffer_size']} \\
    --vfs-read-chunk-size {config['read_chunk']} \\
    --vfs-read-chunk-size-limit 1G \\
    --buffer-size {config['buffer_size']} \\
    --transfers {config['transfers']} \\
    --checkers {config['checkers']} \\
    --timeout {config['timeout']} \\
    --contimeout {config['contimeout']} \\
    --low-level-retries {config['low_level_retries']} \\
    --retries {config['retries']} \\
    --retries-sleep {config['retries_sleep']} \\
    --dir-cache-time {config['dir_cache']} \\
    --attr-timeout {config['attr_timeout']} \\
    --poll-interval 1m \\
    --s3-no-head \\
    --no-checksum \\
    --allow-other \\
    --uid 1000 \\
    --gid 1000 \\
    --log-level INFO \\
    --log-file={log_file} \\
    --stats 5m \\
    --stats-log-level INFO \\
    --daemon
    """)

    execute_script(ip, mount_script, '/tmp/mount_s3.sh')
    print(f"‚úÖ S3 mont√© et accessible par Docker sur {mount_point}")


def start_plex_container(ip, claim_token, version='latest', container_name='plex',
                        config_path='/opt/plex_data/config',
                        media_path='/mnt/s3-media',
                        transcode_path='/opt/plex_data/transcode',
                        memory='4g',
                        memory_swap='6g',
                        cpus='2.0',
                        wait_for_s3=True):
    """
    Lance le conteneur Plex avec des chemins de volumes configurables.
    Version align√©e avec setup-plex-unified.sh fonctionnel.

    Args:
        ip: 'localhost' ou IP remote
        claim_token: Token Plex Claim
        version: Version de Plex (default: 'latest')
        container_name: Nom du conteneur Docker
        config_path: Chemin de configuration Plex
        media_path: Chemin du montage S3
        transcode_path: Chemin de transcodage
        memory: Limite m√©moire Docker (ex: '4g')
        memory_swap: Limite swap Docker (ex: '6g')
        cpus: Nombre de CPUs (ex: '2.0')
        wait_for_s3: Attendre que S3 soit pr√™t avant de lancer Plex
    """
    print(f"üöÄ D√©marrage du conteneur Plex (version: {version})...")

    if wait_for_s3:
        print("   ‚è≥ V√©rification de la disponibilit√© du montage S3...")
        # Attendre que le S3 soit vraiment pr√™t
        s3_ready_script = f"""#!/bin/bash
for i in {{1..30}}; do
    if mountpoint -q {media_path} && timeout 5 ls {media_path} > /dev/null 2>&1; then
        echo "‚úÖ S3 pr√™t apr√®s $i secondes"
        exit 0
    fi
    echo "   ... attente du S3 ($i/30)"
    sleep 2
done
echo "‚ùå Timeout - S3 non pr√™t apr√®s 60 secondes"
exit 1
"""
        execute_script(ip, s3_ready_script, '/tmp/wait_for_s3.sh')

    # V√©rifier que les volumes existent
    print(f"   üìÅ V√©rification des volumes...")
    for path, name in [(config_path, 'config'), (media_path, 'media'), (transcode_path, 'transcode')]:
        result = execute_command(ip, f"test -d {path}", check=False, capture_output=True)
        if result.returncode == 0:
            print(f"   ‚úÖ {name}: {path}")
        else:
            print(f"   ‚ö†Ô∏è  {name}: {path} n'existe pas, cr√©ation...")
            execute_command(ip, f"mkdir -p {path}")

    # Script de lancement align√© avec setup-plex-unified.sh
    launch_script = f"""#!/bin/bash
set -euo pipefail

echo "üßπ Nettoyage d'√©ventuels conteneurs existants..."
docker stop "{container_name}" 2>/dev/null || true
docker rm -f "{container_name}" 2>/dev/null || true

echo "üê≥ Lancement du conteneur Plex..."
docker run -d \\
    --name "{container_name}" \\
    --restart unless-stopped \\
    --memory={memory} \\
    --memory-swap={memory_swap} \\
    --cpus={cpus} \\
    -p 32400:32400 \\
    -e TZ="Europe/Paris" \\
    -e PLEX_UID=1000 \\
    -e PLEX_GID=1000 \\
    -e PLEX_CLAIM="{claim_token}" \\
    -v "{media_path}:/Media:ro" \\
    -v "{config_path}:/config" \\
    -v "{transcode_path}:/transcode" \\
    plexinc/pms-docker:{version}

echo "‚è≥ Attente de stabilisation (60s)..."
sleep 60

echo "‚úÖ Conteneur Plex lanc√©"
docker logs "{container_name}" 2>&1 | tail -20
"""

    print(f"""
docker run -d \\
    --name "{container_name}" \\
    --restart unless-stopped \\
    --memory={memory} \\
    --memory-swap={memory_swap} \\
    --cpus={cpus} \\
    -p 32400:32400 \\
    -e TZ="Europe/Paris" \\
    -e PLEX_UID=1000 \\
    -e PLEX_GID=1000 \\
    -e PLEX_CLAIM="{claim_token}" \\
    -v "{media_path}:/Media:ro" \\
    -v "{config_path}:/config" \\
    -v "{transcode_path}:/transcode" \\
    plexinc/pms-docker:{version}
    """)

    try:
        execute_script(ip, launch_script, '/tmp/start_plex.sh')

    except Exception as e:
        print(f"‚ùå Erreur lors du d√©marrage: {e}")
        # Tenter de r√©cup√©rer les logs pour debug
        print("üîç Tentative de r√©cup√©ration des logs...")
        logs_result = execute_command(ip, f"docker logs {container_name}", check=False, capture_output=True)
        if logs_result.returncode == 0:
            print(f"üìã Logs du conteneur:\n{logs_result.stdout}")
        raise


def configure_plex_via_api(ip, container, plex_token):
    """
    Configure les pr√©f√©rences Plex via API pour d√©sactiver TOUTES les analyses.
    Plus rapide et s√ªr que l'√©dition XML.
    """
    print("‚öôÔ∏è  Configuration des pr√©f√©rences Plex via API (Mode Scan Only)...")

    # Liste des param√®tres critiques pour √©viter le verrouillage DB
    # On d√©sactive tout ce qui consomme du CPU ou √©crit en DB pendant le scan
    params = {
        "GenerateBIFBehavior": "never",           # Pas de thumbnails vid√©o (Gros consommateur)
        "GenerateIntroMarkerBehavior": "never",   # Pas de d√©tection d'intro (Gros consommateur)
        "GenerateChapterBehavior": "never",       # Pas de chapitres
        "LoudnessAnalysisBehavior": "never",      # Pas d'analyse sonique
        "MusicAnalysisBehavior": "never",         # Pas d'analyse musique
        "AutoScan": "0",                          # Pas de scan auto
        "ButlerTaskDeepAnalysis": "0",            # Pas d'analyse profonde programm√©e
        "BackgroundQueueIdlePaused": "1"          # (Ton id√©e) On pause la queue par s√©curit√©
    }

    # Construction de la query string
    query_string = "&".join([f"{k}={v}" for k, v in params.items()])

    # URL de l'endpoint pr√©f√©rences
    url = f"http://localhost:32400/:/prefs?{query_string}"

    # Appel via curl dans le conteneur
    cmd = f"curl -s -X PUT '{url}' -H 'X-Plex-Token: {plex_token}'"

    result = docker_exec(ip, container, cmd, capture_output=True, check=False)

    if result.returncode == 0:
        print("‚úÖ Pr√©f√©rences appliqu√©es avec succ√®s via API")
        # On v√©rifie une cl√© pour √™tre s√ªr
        check_cmd = f"curl -s 'http://localhost:32400/:/prefs' -H 'X-Plex-Token: {plex_token}' | grep 'GenerateIntroMarkerBehavior'"
        docker_exec(ip, container, check_cmd, check=False)
    else:
        print(f"‚ùå Erreur lors de la configuration API : {result.stderr}")
        # Fallback si l'API √©choue : on pr√©vient l'utilisateur
        print("‚ö†Ô∏è  ATTENTION : Le serveur risque de lancer des analyses concurrentes.")


def enable_plex_analysis_via_api(ip, container, plex_token):
    """
    R√©active les t√¢ches d'analyse (Sonic, Loudness) pour la Phase 7.
    """
    print("üß† Configuration des analyses Plex...")

    # On remet les valeurs par d√©faut (ou "scheduled" / "asap")
    params = {
        "GenerateBIFBehavior": "scheduled",       # Thumbnails
        "GenerateIntroMarkerBehavior": "asap",    # Intros
        "GenerateChapterBehavior": "scheduled",   # Chapitres
        "LoudnessAnalysisBehavior": "asap",       # Analyse sonique (CRUCIAL pour vous)
        "MusicAnalysisBehavior": "asap",          # Analyse musique
        "AutoScan": "0",                          # On garde l'autoscan OFF
        "ButlerTaskDeepAnalysis": "1",            # On r√©active le Butler
        "BackgroundQueueIdlePaused": "0"          # On relance la queue
    }

    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
    url = f"http://localhost:32400/:/prefs?{query_string}"

    cmd = f"curl -s -X PUT '{url}' -H 'X-Plex-Token: {plex_token}'"
    result = docker_exec(ip, container, cmd, capture_output=True, check=False)

    if result.returncode == 0:
        print("‚úÖ Pr√©f√©rences d'analyse configur√©es.")

        # D√©clencher le Butler pour lancer les analyses imm√©diatement
        # Note (2026-01-23): Sans cet appel, les pr√©f√©rences "asap" ne d√©clenchent
        # PAS l'analyse sur les items existants. Le Butler doit √™tre appel√©
        # explicitement pour scanner la biblioth√®que et lancer les t√¢ches.
        butler_cmd = f"curl -s -X POST 'http://localhost:32400/butler/DeepMediaAnalysis' -H 'X-Plex-Token: {plex_token}'"
        docker_exec(ip, container, butler_cmd, check=False)
        print("‚úÖ Butler DeepMediaAnalysis d√©clench√©.")
    else:
        print(f"‚ùå Erreur r√©activation : {result.stderr}")


def debug_plex_container(ip, container='plex'):
    """Fonction de debug pour investiguer l'√©tat du conteneur Plex"""
    debug_script = f"""#!/bin/bash
echo "=== √âtat du conteneur ==="
docker ps -a --filter name={container}
echo ""
echo "=== Logs du conteneur ==="
docker logs {container} || echo "Aucun log disponible"
echo ""
echo "=== Inspection d√©taill√©e ==="
docker inspect {container} || echo "Conteneur non trouv√©"
echo ""
echo "=== Processus Docker ==="
docker ps -a
echo ""
echo "=== Espace disque ==="
df -h
echo ""
echo "=== Permissions des volumes ==="
ls -la /opt/plex_data/ /mnt/s3-media/ 2>/dev/null || echo "Certains volumes non accessibles"
"""
    execute_script(ip, debug_script, '/tmp/debug_plex.sh')


def get_plex_token(ip, container='plex', timeout=120, retry_interval=10):
    """
    R√©cup√®re le token d'authentification Plex depuis le conteneur avec retry.
    Le token est stock√© dans /config/Library/Application Support/Plex Media Server/Preferences.xml

    Le token peut mettre du temps √† appara√Ætre apr√®s le claim initial,
    d'o√π le m√©canisme de retry.

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        timeout: Temps max d'attente en secondes (default: 120s)
        retry_interval: Intervalle entre les tentatives (default: 10s)

    Returns:
        str: Token Plex (X-Plex-Token) ou None si √©chec
    """
    print("üîë R√©cup√©ration du token Plex...")
    start_time = time.time()

    # Extraire le token depuis le fichier Preferences.xml
    cmd = (
        f"docker exec {container} cat '/config/Library/Application Support/Plex Media Server/Preferences.xml' "
        f"| grep -oP 'PlexOnlineToken=\"\\K[^\"]+'"
    )

    while time.time() - start_time < timeout:
        elapsed = int(time.time() - start_time)
        result = execute_command(ip, cmd, capture_output=True, check=False)

        if result.returncode == 0 and result.stdout.strip():
            token = result.stdout.strip()
            print(f"   ‚úÖ Token r√©cup√©r√© apr√®s {elapsed}s : {token[:20]}...")
            return token

        # Log de progression
        if elapsed > 0 and elapsed % 30 == 0:
            print(f"   ‚è≥ Token non disponible... ({elapsed}s/{timeout}s)")

        time.sleep(retry_interval)

    print(f"‚ö†Ô∏è  Impossible de r√©cup√©rer le token Plex apr√®s {timeout}s")
    return None


def wait_plex_ready(ip, container='plex', timeout=180):
    """
    Attend que Plex soit pr√™t (conteneur healthy + API r√©pond).
    """
    print("‚è≥ Attente du d√©marrage complet de Plex...")

    start_time = time.time()

    while time.time() - start_time < timeout:
        elapsed = int(time.time() - start_time)

        # 1. V√©rifier si le conteneur est en cours d'ex√©cution
        check_cmd = f"docker inspect --format '{{{{.State.Running}}}}' {container} 2>/dev/null"
        docker_result = execute_command(ip, check_cmd, check=False, capture_output=True)

        if docker_result.returncode != 0 or docker_result.stdout.strip() != 'true':
            if elapsed % 10 == 0:  # Log toutes les 10s
                print(f"   ‚è≥ [{elapsed}s] Conteneur pas encore d√©marr√©...")
            time.sleep(5)
            continue

        # 2. V√©rifier l'API Plex via docker exec curl (plus fiable que requests)
        api_check = f"docker exec {container} curl -s http://localhost:32400/identity 2>/dev/null"
        api_result = execute_command(ip, api_check, check=False, capture_output=True)

        if api_result.returncode == 0 and 'MediaContainer' in api_result.stdout:
            print(f"‚úÖ Plex est pr√™t (API r√©pond apr√®s {elapsed}s)")
            return

        # Log de progression toutes les 30s
        if elapsed > 0 and elapsed % 30 == 0:
            print(f"   ‚è≥ API Plex pas encore pr√™te... ({elapsed}s/{timeout}s)")

        time.sleep(5)

    raise TimeoutError(f"‚ùå Plex n'a pas d√©marr√© dans les {timeout}s")


def wait_plex_fully_ready(ip, container='plex', timeout=300):
    """
    Attend que Plex soit compl√®tement pr√™t (serveur claim√© + plugins charg√©s).

    Crit√®res de validation :
    1. L'API /identity r√©pond HTTP 200 avec claimed="1"
    2. Au moins 3 processus Plex actifs dans le conteneur

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        timeout: Temps max d'attente en secondes (default: 300s)

    Returns:
        bool: True si Plex est pr√™t, False sinon
    """
    print("‚è≥ Attente que Plex soit compl√®tement initialis√©...")
    start_time = time.time()
    last_api_status = "inconnu"
    last_api_response = ""
    is_claimed = False

    while time.time() - start_time < timeout:
        elapsed = int(time.time() - start_time)

        # V√©rifier que Plex r√©pond ET que le serveur est claim√©
        cmd = "curl -s -w '\\n%{http_code}' http://localhost:32400/identity 2>&1"
        result = docker_exec(ip, container, cmd, capture_output=True, check=False)

        # Parser la r√©ponse : body + code HTTP
        lines = result.stdout.strip().split('\n')
        http_code = lines[-1] if lines else "0"
        api_body = '\n'.join(lines[:-1]) if len(lines) > 1 else ""
        last_api_response = api_body[:200]

        # Analyser le statut de l'API - v√©rifier le claim
        if http_code == "200" and 'claimed="1"' in api_body:
            last_api_status = "OK (claim√©)"
            is_claimed = True
        elif http_code == "200" and 'claimed="0"' in api_body:
            last_api_status = "Non claim√© (PLEX_CLAIM invalide/expir√©?)"
            is_claimed = False
        elif http_code == "200":
            last_api_status = "HTTP 200 (statut claim inconnu)"
            is_claimed = False
        elif http_code.isdigit() and int(http_code) > 0:
            last_api_status = f"HTTP {http_code}"
            is_claimed = False
        else:
            last_api_status = "pas de r√©ponse"
            is_claimed = False

        # V√©rifier aussi les processus syst√®me Plex
        processes_cmd = "ps aux | grep -i plex | grep -v grep | wc -l"
        processes_result = docker_exec(ip, container, processes_cmd, capture_output=True, check=False)
        plex_processes = int(processes_result.stdout.strip()) if processes_result.stdout.strip().isdigit() else 0

        # Crit√®re de succ√®s : serveur claim√© + au moins 3 processus
        if is_claimed and plex_processes >= 3:
            print(f"‚úÖ Plex compl√®tement initialis√© apr√®s {elapsed}s")
            return True

        # Log de progression avec d√©tails
        print(f"   [{elapsed}s] Processus: {plex_processes}, API: {last_api_status}")
        time.sleep(30)

    # Timeout atteint - diagnostic d√©taill√©
    print(f"\n‚ö†Ô∏è  Plex n'est pas compl√®tement initialis√© apr√®s {timeout}s")
    print(f"   Dernier statut API : {last_api_status}")
    if last_api_response:
        print(f"   Derni√®re r√©ponse   : {last_api_response[:100]}...")

    # Message d'aide si non claim√©
    if not is_claimed and 'claimed="0"' in last_api_response:
        print("\nüí° Le serveur n'est pas claim√©. Causes possibles :")
        print("   ‚Ä¢ PLEX_CLAIM expir√© (dur√©e de vie ~4 minutes)")
        print("   ‚Ä¢ PLEX_CLAIM d√©j√† utilis√©")
        print("   ‚Ä¢ Probl√®me r√©seau vers plex.tv")
        print("   ‚Üí G√©n√©rez un nouveau claim : https://www.plex.tv/claim")

    # Capturer les logs Docker pour diagnostic
    print("\nüìã Logs Docker (derni√®res 20 lignes) :")
    logs_cmd = f"docker logs --tail 20 {container} 2>&1"
    logs_result = execute_command(ip, logs_cmd, capture_output=True, check=False)
    if logs_result.stdout:
        for line in logs_result.stdout.strip().split('\n')[:20]:
            print(f"   {line}")

    return False


def wait_plex_ready_for_libraries(ip, container, plex_token, timeout=120):
    """Attendre que Plex soit VRAIMENT pr√™t pour cr√©er des biblioth√®ques"""
    print("‚è≥ Attente que Plex soit pr√™t pour les biblioth√®ques...")

    start = time.time()
    while time.time() - start < timeout:
        # Test si on peut cr√©er une biblioth√®que test
        cmd = f"curl -s 'http://localhost:32400/library/sections' -H 'X-Plex-Token: {plex_token}'"
        result = docker_exec(ip, container, cmd, capture_output=True, check=False)

        # Si la r√©ponse contient "MediaContainer" sans erreur "starting"
        if "MediaContainer" in result.stdout and "starting" not in result.stdout:
            print("‚úÖ Plex pr√™t")
            time.sleep(5)  # S√©curit√© suppl√©mentaire
            return True

        time.sleep(5)

    print("‚ö†Ô∏è Timeout")
    return False


def add_library(ip, container, library_config, plex_token):
    """Version ultra simple pour debug"""
    title = library_config['title']
    print(f"üìö Ajout de '{title}'...")

    # Utiliser une URL simple avec param√®tres GET (plus fiable que POST avec -F)
    from urllib.parse import quote

    params = {
        'name': title,
        'type': library_config['type'],
        'agent': library_config.get('agent', 'tv.plex.agents.movie'),
        'scanner': library_config.get('scanner', 'Plex Movie'),
        'language': library_config.get('language', 'fr-FR'),
        'location': library_config['paths'][0] if isinstance(library_config['paths'], list) else library_config['paths']
    }

    # # Utiliser l'agent "Plex Series" (Legacy) pour les s√©ries TV
    # if params['type'] == 'show':
    #     # On utilise l'ancien agent plus permissif pour le test
    #     params['agent'] = "com.plexapp.agents.thetvdb"
    #     params['scanner'] = "Plex Series Scanner"
    #     params['language'] = "fr-FR"

    # Construire l'URL avec param√®tres
    url_params = '&'.join([f"{k}={quote(str(v))}" for k, v in params.items()])
    curl_cmd = f"curl -X POST 'http://localhost:32400/library/sections?{url_params}&X-Plex-Token={plex_token}'"

    result = docker_exec(ip, container, curl_cmd, capture_output=True, check=False)

    # Debug : toujours voir ce qui se passe
    if result.stdout:
        print(f"   Location: {params['location']}")
        print(f"   R√©ponse: {result.stdout[:200]}")
    if result.stderr:
        print(f"   Erreur: {result.stderr[:200]}")

    # V√©rifier imm√©diatement
    time.sleep(2)
    verify_cmd = f"curl -s 'http://localhost:32400/library/sections' -H 'X-Plex-Token: {plex_token}'"
    verify_result = docker_exec(ip, container, verify_cmd, capture_output=True, check=False)

    if title in verify_result.stdout:
        print(f"   ‚úÖ '{title}' cr√©√©e")
        return True
    else:
        print(f"   ‚ùå '{title}' non trouv√©e")
        return False


def create_library_section(ip, container, library_config, plex_token):
    """Juste la cr√©ation via curl"""
    form_data = [
        f"name={library_config['title']}",
        f"type={library_config['type']}",
        f"agent={library_config.get('agent', 'tv.plex.agents.movie')}",
        f"scanner={library_config.get('scanner', 'Plex Movie')}",
        f"language={library_config.get('language', 'fr-FR')}",
    ]

    for path in library_config.get('paths', []):
        form_data.append(f"location={path}")

    form_string = " ".join([f"-F '{data}'" for data in form_data])
    headers = f"-H 'X-Plex-Token: {plex_token}'" if plex_token else ""

    curl_cmd = f"curl -sX POST http://localhost:32400/library/sections {form_string} {headers}"
    result = docker_exec(ip, container, curl_cmd, capture_output=True, check=False)

    if result.returncode != 0 or "error" in result.stdout.lower():
        print(f"   ‚ùå Erreur: {result.stderr or result.stdout}")
        return False
    return True


def wait_library_visible(ip, container, title, plex_token, max_wait=30):
    """Attendre que la biblioth√®que soit visible dans l'API"""
    print(f"   ‚è≥ Attente visibilit√©...")

    for i in range(max_wait):
        time.sleep(1)

        cmd = f"curl -s 'http://localhost:32400/library/sections' -H 'X-Plex-Token: {plex_token}'"
        result = docker_exec(ip, container, cmd, capture_output=True, check=False)

        # Chercher la section
        match = re.search(rf'key="(\d+)"[^>]*title="{re.escape(title)}"', result.stdout)
        if match:
            return match.group(1)

    print(f"   ‚ö†Ô∏è Timeout apr√®s {max_wait}s")
    return None


def prewarm_rclone_cache(ip, mount_point, max_depth=3):
    """
    Pr√©chauffe le cache rclone via Python (plus verbeux et contr√¥lable).
    """
    print(f"üî• [CACHE] D√©marrage du pr√©chauffage sur {mount_point}...")

    if ip != 'localhost':
        # En remote, on garde 'find' car c'est plus rapide √† envoyer via SSH
        # mais on retire > /dev/null pour voir si √ßa bloque
        cmd = f"find {mount_point} -maxdepth {max_depth} -type d | head -n 5 && echo '... (suite masqu√©e)'"
        execute_command(ip, cmd, check=False)
        return

    # === VERSION LOCALE (PYTHON) ===
    start_time = time.time()
    dir_count = 0

    # Normaliser le chemin pour calculer la profondeur
    base_path = str(mount_point).rstrip(os.sep)
    base_depth = base_path.count(os.sep)

    try:
        for root, dirs, files in os.walk(base_path):
            # Calcul de la profondeur actuelle
            current_depth = root.count(os.sep) - base_depth

            # Feedback visuel toutes les 10 entr√©es pour ne pas spammer
            dir_count += 1
            if dir_count % 10 == 0:
                elapsed = int(time.time() - start_time)
                print(f"   ‚è≥ [{elapsed}s] Scann√©s: {dir_count} dossiers | Actuel: {os.path.basename(root)}", end='\r')

            # Si on atteint la profondeur max, on vide 'dirs' pour emp√™cher os.walk de descendre plus bas
            if current_depth >= max_depth:
                del dirs[:]

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Pr√©chauffage interrompu par l'utilisateur")
    except Exception as e:
        print(f"\n‚ùå Erreur durant le pr√©chauffage : {e}")

    total_time = time.time() - start_time
    print(f"\n‚úÖ [CACHE] Termin√© : {dir_count} dossiers visit√©s en {total_time:.1f}s")


def disable_all_background_tasks(ip, container, plex_token):
    """
    D√©sactive toutes les t√¢ches de fond Plex pour √©viter la comp√©tition durant le scan.
    Met Plex en "mode silencieux" pour lib√©rer les ressources.

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        plex_token: Token d'authentification Plex

    Returns:
        bool: True si succ√®s, False sinon
    """
    print("üîá D√©sactivation de toutes les t√¢ches de fond...")

    params = {
        "GenerateBIFBehavior": "never",
        "GenerateIntroMarkerBehavior": "never",
        "GenerateChapterBehavior": "never",
        "LoudnessAnalysisBehavior": "never",
        "MusicAnalysisBehavior": "never",
        "ButlerTaskDeepAnalysis": "0",
        "ButlerTaskGenerateAutoTags": "0",
        "ButlerTaskRefreshLibraries": "0",
        "ButlerTaskRefreshLocalMedia": "0",
        "ButlerTaskRefreshPeriodicMetadata": "0",
        "BackgroundQueueIdlePaused": "1",
        "AutoScan": "0"
    }

    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
    url = f"http://localhost:32400/:/prefs?{query_string}"
    cmd = f"curl -s -X PUT '{url}' -H 'X-Plex-Token: {plex_token}'"

    result = docker_exec(ip, container, cmd, capture_output=True, check=False)

    if result.returncode == 0:
        print("   ‚úÖ T√¢ches de fond d√©sactiv√©es")
        return True
    else:
        print(f"   ‚ùå Erreur: {result.stderr}")
        return False


def enable_music_analysis_only(ip, container, plex_token):
    """
    Active UNIQUEMENT les analyses musicales (Sonic/Loudness), tout le reste reste d√©sactiv√©.
    Utilis√© pour la phase de traitement Sonic en isolation.

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        plex_token: Token d'authentification Plex

    Returns:
        bool: True si succ√®s, False sinon
    """
    print("üéπ Activation exclusive des analyses musicales...")

    params = {
        "LoudnessAnalysisBehavior": "asap",
        "MusicAnalysisBehavior": "asap",
        "BackgroundQueueIdlePaused": "0",
        # Activer Deep Analysis (n√©cessaire pour Sonic)
        "ButlerTaskDeepAnalysis": "1",
        # Tout le reste reste √† "never"
        "GenerateBIFBehavior": "never",
        "GenerateIntroMarkerBehavior": "never",
        "GenerateChapterBehavior": "never"
    }

    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
    url = f"http://localhost:32400/:/prefs?{query_string}"
    cmd = f"curl -s -X PUT '{url}' -H 'X-Plex-Token: {plex_token}'"

    result = docker_exec(ip, container, cmd, capture_output=True, check=False)

    if result.returncode == 0:
        print("   ‚úÖ Analyses musicales activ√©es")

        # D√©clencher le Butler pour lancer l'analyse Sonic imm√©diatement
        # Note (2026-01-23): Sans cet appel explicite, les pr√©f√©rences "asap"
        # n'ont aucun effet sur les items d√©j√† pr√©sents. Le Butler scanne la
        # biblioth√®que et cr√©e les t√¢ches d'analyse pour chaque piste.
        # R√©f: forums.plex.tv/t/sonic-analysis-doesn-t-trigger
        butler_cmd = f"curl -s -X POST 'http://localhost:32400/butler/DeepMediaAnalysis' -H 'X-Plex-Token: {plex_token}'"
        butler_result = docker_exec(ip, container, butler_cmd, capture_output=True, check=False)
        if butler_result.returncode == 0:
            print("   ‚úÖ Butler DeepMediaAnalysis d√©clench√©")
        else:
            print(f"   ‚ö†Ô∏è  Butler non d√©clench√©: {butler_result.stderr}")

        return True
    else:
        print(f"   ‚ùå Erreur: {result.stderr}")
        return False


def enable_all_analysis(ip, container, plex_token):
    """
    R√©active toutes les analyses pour le clean-up final (Photos, Vid√©os).

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        plex_token: Token d'authentification Plex

    Returns:
        bool: True si succ√®s, False sinon
    """
    print("üîÑ R√©activation de toutes les analyses...")

    params = {
        "GenerateBIFBehavior": "scheduled",
        "GenerateIntroMarkerBehavior": "scheduled",
        "GenerateChapterBehavior": "scheduled",
        "LoudnessAnalysisBehavior": "scheduled",
        "MusicAnalysisBehavior": "scheduled",
        "ButlerTaskDeepAnalysis": "1",
        "BackgroundQueueIdlePaused": "0"
    }

    query_string = "&".join([f"{k}={v}" for k, v in params.items()])
    url = f"http://localhost:32400/:/prefs?{query_string}"
    cmd = f"curl -s -X PUT '{url}' -H 'X-Plex-Token: {plex_token}'"

    result = docker_exec(ip, container, cmd, capture_output=True, check=False)

    if result.returncode == 0:
        print("   ‚úÖ Toutes les analyses r√©activ√©es")
        return True
    else:
        print(f"   ‚ùå Erreur: {result.stderr}")
        return False


def collect_plex_logs(ip, container, output_dir="logs", prefix="plex", terminal_log=None, rclone_log=None, timestamp=None, keep_terminal_log=False):
    """
    R√©cup√®re les logs Plex pour debug, avec option d'inclure le log terminal et rclone.

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        output_dir: R√©pertoire de destination des logs (d√©faut: logs/)
        prefix: Pr√©fixe pour le nom d'archive
        terminal_log: Chemin vers le fichier log terminal (optionnel)
        rclone_log: Chemin vers le fichier log rclone (optionnel, sur l'h√¥te distant ou local)
        timestamp: Horodatage √† utiliser (optionnel, g√©n√©r√© si non fourni)
        keep_terminal_log: Si True, ne pas supprimer le fichier terminal apr√®s archivage
                          (utile si TeeLogger est encore actif)

    Returns:
        str: Chemin de l'archive ou None si √©chec
    """
    import tarfile
    import tempfile
    import shutil

    # Cr√©er le dossier de destination s'il n'existe pas
    os.makedirs(output_dir, exist_ok=True)

    if timestamp is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    archive_name = f"{timestamp}_logs_{prefix}.tar.gz"

    print(f"üìã Collecte des logs dans {output_dir}/...")

    if ip == 'localhost':
        archive_path = os.path.join(output_dir, archive_name)
    else:
        archive_path = f"/tmp/{archive_name}"

    # Copier tous les fichiers .log depuis le conteneur (collecte dynamique)
    log_path = "/config/Library/Application Support/Plex Media Server/Logs"
    tar_cmd = f"""docker exec {container} sh -c "cd '{log_path}' && tar -czf /tmp/plex_logs.tar.gz *.log 2>/dev/null || echo 'Aucun log trouv√©'" """

    result = execute_command(ip, tar_cmd, check=False, capture_output=True)

    if ip == 'localhost':
        # Copier depuis le conteneur vers l'h√¥te
        execute_command(ip, f"docker cp {container}:/tmp/plex_logs.tar.gz {archive_path}", check=False)
    else:
        # D√©placer l'archive sur le remote
        execute_command(ip, f"mv /tmp/plex_logs.tar.gz {archive_path}", check=False)

    # V√©rifier que l'archive Plex existe
    plex_archive_exists = False
    if ip == 'localhost':
        plex_archive_exists = os.path.exists(archive_path)
    else:
        check_result = execute_command(ip, f"test -f {archive_path} && echo 'OK'", capture_output=True, check=False)
        plex_archive_exists = 'OK' in check_result.stdout

    if not plex_archive_exists:
        print(f"   ‚ö†Ô∏è  √âchec de la collecte des logs Plex")
        # Si on a un terminal_log, on peut quand m√™me cr√©er une archive avec juste ce log
        if terminal_log and os.path.exists(terminal_log):
            terminal_only_archive = os.path.join(output_dir, f"{timestamp}_terminal_{prefix}.tar.gz")
            with tarfile.open(terminal_only_archive, 'w:gz') as tar:
                tar.add(terminal_log, arcname='terminal.log')
            # Supprimer le fichier terminal brut (maintenant dans l'archive)
            if not keep_terminal_log:
                os.remove(terminal_log)
            size_mb = os.path.getsize(terminal_only_archive) / (1024*1024)
            print(f"   ‚úÖ Log terminal seul: {terminal_only_archive} ({size_mb:.1f} MB)")
            return terminal_only_archive
        return None

    # V√©rifier si on a des logs suppl√©mentaires √† inclure
    has_terminal = terminal_log and os.path.exists(terminal_log)
    has_rclone = False
    local_rclone_log = None

    # Si rclone_log est sp√©cifi√©, v√©rifier s'il existe (local ou remote)
    if rclone_log:
        if ip == 'localhost':
            has_rclone = os.path.exists(rclone_log)
            local_rclone_log = rclone_log if has_rclone else None
        else:
            # V√©rifier si le fichier existe sur le remote
            check_result = execute_command(ip, f"test -f {rclone_log} && echo 'OK'", capture_output=True, check=False)
            has_rclone = 'OK' in check_result.stdout

    # Si pas de logs suppl√©mentaires, retourner l'archive Plex seule
    if not has_terminal and not has_rclone:
        if ip == 'localhost':
            size_mb = os.path.getsize(archive_path) / (1024*1024)
            print(f"   ‚úÖ Logs Plex collect√©s: {archive_path} ({size_mb:.1f} MB)")
        else:
            print(f"   ‚úÖ Logs Plex collect√©s: {archive_path}")
        return archive_path

    # Cr√©er une archive combin√©e (Plex + terminal + rclone)
    extras = []
    if has_terminal:
        extras.append("terminal")
    if has_rclone:
        extras.append("rclone")
    print(f"   üì¶ Cr√©ation archive combin√©e (Plex + {' + '.join(extras)})...")

    combined_archive = os.path.join(output_dir, f"{timestamp}_logs_{prefix}_all.tar.gz")
    temp_dir = tempfile.mkdtemp(prefix="plex_logs_")

    try:
        # Extraire l'archive Plex dans un sous-dossier
        plex_logs_dir = os.path.join(temp_dir, "plex_logs")
        os.makedirs(plex_logs_dir)

        if ip == 'localhost':
            with tarfile.open(archive_path, 'r:gz') as tar:
                tar.extractall(plex_logs_dir)
        else:
            # T√©l√©charger l'archive remote d'abord
            local_plex_archive = os.path.join(temp_dir, "plex_logs.tar.gz")
            execute_command(
                'localhost',
                f"scp -o StrictHostKeyChecking=no root@{ip}:{archive_path} {local_plex_archive}",
                check=False
            )
            if os.path.exists(local_plex_archive):
                with tarfile.open(local_plex_archive, 'r:gz') as tar:
                    tar.extractall(plex_logs_dir)

        # Si rclone_log est sur un remote, le t√©l√©charger
        if has_rclone and ip != 'localhost':
            local_rclone_log = os.path.join(temp_dir, "rclone.log")
            execute_command(
                'localhost',
                f"scp -o StrictHostKeyChecking=no root@{ip}:{rclone_log} {local_rclone_log}",
                check=False
            )
            if not os.path.exists(local_rclone_log):
                local_rclone_log = None
                has_rclone = False

        # Cr√©er l'archive combin√©e
        with tarfile.open(combined_archive, 'w:gz') as tar:
            # Ajouter les logs Plex
            for item in os.listdir(plex_logs_dir):
                tar.add(os.path.join(plex_logs_dir, item), arcname=f"plex_logs/{item}")
            # Ajouter le log terminal
            if has_terminal:
                tar.add(terminal_log, arcname='terminal.log')
            # Ajouter le log rclone
            if has_rclone and local_rclone_log and os.path.exists(local_rclone_log):
                tar.add(local_rclone_log, arcname='rclone.log')

        # Supprimer l'archive Plex seule (remplac√©e par la combin√©e)
        if ip == 'localhost' and os.path.exists(archive_path):
            os.remove(archive_path)

        # Supprimer le fichier terminal brut (maintenant dans l'archive)
        if not keep_terminal_log and terminal_log and os.path.exists(terminal_log):
            os.remove(terminal_log)

        size_mb = os.path.getsize(combined_archive) / (1024*1024)
        print(f"   ‚úÖ Archive combin√©e: {combined_archive} ({size_mb:.1f} MB)")
        return combined_archive

    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur archive combin√©e: {e}")
        # Fallback: retourner l'archive Plex seule
        if ip == 'localhost' and os.path.exists(archive_path):
            size_mb = os.path.getsize(archive_path) / (1024*1024)
            print(f"   ‚úÖ Logs Plex (fallback): {archive_path} ({size_mb:.1f} MB)")
            return archive_path
        return None
    finally:
        # Nettoyage du r√©pertoire temporaire
        shutil.rmtree(temp_dir, ignore_errors=True)


def stop_plex(ip, container='plex', timeout=30):
    """
    Arr√™te proprement le conteneur Plex avec timeout et fallback

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur
        timeout: Timeout en secondes avant force kill (d√©faut: 30)

    Cette fonction tente d'abord un arr√™t gracieux avec timeout.
    Si √ßa √©choue, elle force l'arr√™t avec docker kill.
    """
    print(f"‚è∏Ô∏è  Arr√™t du conteneur {container}...")

    # Essayer stop avec timeout
    result = execute_command(
        ip,
        f"docker stop -t {timeout} {container}",
        check=False,
        capture_output=True
    )

    if result.returncode != 0:
        print(f"   ‚ö†Ô∏è  Stop timeout, tentative de kill...")
        execute_command(ip, f"docker kill {container}", check=False)

    print(f"‚úÖ Conteneur {container} arr√™t√©")


def verify_rclone_mount_healthy_simple(ip, mount_point='/mnt/s3-media', timeout=30):
    """
    V√©rifie que le montage rclone est fonctionnel (version simple sans lecture fichier).

    Cette version ne fait que les tests 1 et 2 (mountpoint + ls).
    Utilis√©e apr√®s un remontage pour √©viter les faux n√©gatifs avec MEGA.

    Args:
        ip: 'localhost' ou IP remote
        mount_point: Point de montage √† v√©rifier
        timeout: Timeout en secondes pour le test d'acc√®s

    Returns:
        dict: {
            'healthy': bool,
            'error': str|None,
            'response_time': float
        }
    """
    import time as time_module
    start = time_module.time()

    # Test 1: V√©rifier que le point de montage existe
    result = execute_command(ip, f"mountpoint -q {mount_point}", check=False, capture_output=True)
    if result.returncode != 0:
        return {
            'healthy': False,
            'error': f"Point de montage {mount_point} non actif",
            'response_time': time_module.time() - start
        }

    # Test 2: V√©rifier l'acc√®s avec timeout (d√©tecte les sockets morts)
    test_cmd = f"timeout {timeout} ls {mount_point} > /dev/null 2>&1"
    result = execute_command(ip, test_cmd, check=False, capture_output=True)

    response_time = time_module.time() - start

    if result.returncode == 124:  # Timeout
        return {
            'healthy': False,
            'error': f"Timeout ({timeout}s) - socket probablement d√©connect√©",
            'response_time': response_time
        }
    elif result.returncode != 0:
        return {
            'healthy': False,
            'error': f"Erreur d'acc√®s au montage (code {result.returncode})",
            'response_time': response_time
        }

    return {
        'healthy': True,
        'error': None,
        'response_time': response_time
    }


def verify_rclone_mount_healthy(ip, mount_point='/mnt/s3-media', timeout=30):
    """
    V√©rifie que le montage rclone est fonctionnel (pas de socket d√©connect√©).

    Effectue 3 tests progressifs:
    1. V√©rification que le point de montage est actif
    2. Lecture du contenu du r√©pertoire (metadata)
    3. Lecture r√©elle d'un fichier (d√©tecte les I/O bloqu√©s silencieusement)

    Args:
        ip: 'localhost' ou IP remote
        mount_point: Point de montage √† v√©rifier
        timeout: Timeout en secondes pour le test d'acc√®s (d√©faut: 30s pour MEGA)

    Returns:
        dict: {
            'healthy': bool,      # True si le montage est fonctionnel
            'error': str|None,    # Message d'erreur si √©chec
            'response_time': float # Temps de r√©ponse en secondes
        }
    """
    import time as time_module
    start = time_module.time()

    # Test 1: V√©rifier que le point de montage existe
    result = execute_command(ip, f"mountpoint -q {mount_point}", check=False, capture_output=True)
    if result.returncode != 0:
        return {
            'healthy': False,
            'error': f"Point de montage {mount_point} non actif",
            'response_time': time_module.time() - start
        }

    # Test 2: V√©rifier l'acc√®s avec timeout (d√©tecte les sockets morts)
    test_cmd = f"timeout {timeout} ls {mount_point} > /dev/null 2>&1"
    result = execute_command(ip, test_cmd, check=False, capture_output=True)

    if result.returncode == 124:  # Timeout
        return {
            'healthy': False,
            'error': f"Timeout ({timeout}s) - socket probablement d√©connect√©",
            'response_time': time_module.time() - start
        }
    elif result.returncode != 0:
        return {
            'healthy': False,
            'error': f"Erreur d'acc√®s au montage (code {result.returncode})",
            'response_time': time_module.time() - start
        }

    # Test 3: Lecture r√©elle d'un fichier (pas juste metadata)
    # D√©tecte les cas o√π ls passe mais l'I/O est bloqu√© silencieusement
    # Note: On limite la recherche √† maxdepth 3 pour √©viter un scan complet du bucket
    # et on cherche dans Music/ en priorit√© (r√©pertoire le plus utilis√©)
    test_read_cmd = f"""timeout {timeout} sh -c '
        # Essayer d abord dans Music (sous-r√©pertoire courant)
        file=$(find {mount_point}/Music -maxdepth 3 -type f \\( -name "*.mp3" -o -name "*.flac" -o -name "*.m4a" \\) 2>/dev/null | head -1)
        # Fallback sur tout le mount si Music n existe pas
        if [ -z "$file" ]; then
            file=$(find {mount_point} -maxdepth 3 -type f \\( -name "*.mp3" -o -name "*.flac" -o -name "*.m4a" -o -name "*.mp4" -o -name "*.mkv" \\) 2>/dev/null | head -1)
        fi
        if [ -n "$file" ]; then
            head -c 100 "$file" > /dev/null 2>&1
            echo "OK: $file"
        else
            # Pas de fichier trouv√©, mais le mount semble OK
            echo "OK: no_file_found"
        fi
    '"""
    result = execute_command(ip, test_read_cmd, check=False, capture_output=True)

    response_time = time_module.time() - start

    if result.returncode == 124:  # Timeout sur la lecture
        return {
            'healthy': False,
            'error': f"Timeout lecture fichier ({timeout}s) - I/O bloqu√©",
            'response_time': response_time
        }

    return {
        'healthy': True,
        'error': None,
        'response_time': response_time
    }


def remount_s3_if_needed(ip, rclone_remote, profile='lite', mount_point='/mnt/s3-media',
                         cache_dir=None, log_file=None, config_path=None, max_retries=3,
                         skip_lock=False):
    """
    V√©rifie le montage rclone et remonte si n√©cessaire.

    Args:
        ip: 'localhost' ou IP remote
        rclone_remote: Nom du bucket/chemin S3
        profile: Profil de configuration
        mount_point: Point de montage
        cache_dir: R√©pertoire de cache rclone
        log_file: Fichier de logs rclone
        config_path: Chemin du fichier rclone.conf
        max_retries: Nombre max de tentatives de remontage
        skip_lock: Si True, ne pas acqu√©rir le lock global (appel√© depuis MountMonitor)

    Returns:
        bool: True si le montage est fonctionnel, False si √©chec apr√®s retries
    """
    # Importer le lock global du MountMonitor (import tardif pour √©viter les d√©pendances circulaires)
    from common.mount_monitor import MountHealthMonitor
    global_lock = MountHealthMonitor.get_global_lock()

    # V√©rification initiale (version simple pour √©viter blocage sur FUSE mort)
    health = verify_rclone_mount_healthy_simple(ip, mount_point)
    if health['healthy']:
        return True

    print(f"‚ö†Ô∏è  Montage rclone d√©faillant: {health['error']}")

    # Acqu√©rir le lock global si n√©cessaire (√©vite les remontages concurrents)
    lock_acquired = False
    if not skip_lock:
        if not global_lock.acquire(blocking=False):
            print(f"‚è≥ Remontage d√©j√† en cours par MountMonitor, attente...")
            # Attendre que le MountMonitor finisse son remontage
            global_lock.acquire(blocking=True)
            global_lock.release()
            # Re-v√©rifier apr√®s l'attente (version simple)
            health = verify_rclone_mount_healthy_simple(ip, mount_point)
            if health['healthy']:
                print(f"‚úÖ Montage restaur√© par MountMonitor")
                return True
            # Si toujours pas OK, on continue avec notre propre remontage
            global_lock.acquire(blocking=True)
        lock_acquired = True

    try:
        # Auto-d√©tection du cache_dir si non fourni
        if cache_dir is None:
            if ip == 'localhost':
                cache_dir = os.path.expanduser('~/tmp/rclone-cache')
            else:
                cache_dir = '/mnt/rclone-cache'

        for attempt in range(1, max_retries + 1):
            print(f"üîÑ Tentative de remontage {attempt}/{max_retries}...")

            # D√©montage forc√©
            execute_command(ip, f"pkill -9 -f 'rclone mount.*{mount_point}' || true", check=False)
            time.sleep(3)
            execute_command(ip, f"fusermount3 -uz {mount_point} || true", check=False)
            execute_command(ip, f"fusermount -uz {mount_point} || true", check=False)
            time.sleep(2)

            # Nettoyage du cache VFS rclone (peut contenir des handles corrompus)
            print(f"   üßπ Nettoyage du cache rclone...")
            execute_command(ip, f"rm -rf {cache_dir}/vfs/* 2>/dev/null || true", check=False)

            # Cooldown pour laisser MEGA r√©cup√©rer (augmente avec chaque tentative)
            cooldown = 10 * attempt  # 10s, 20s, 30s
            print(f"   ‚è≥ Cooldown {cooldown}s avant remontage...")
            time.sleep(cooldown)

            # Remontage
            try:
                mount_s3(ip, rclone_remote, profile=profile, mount_point=mount_point,
                         cache_dir=cache_dir, log_file=log_file, config_path=config_path)
            except Exception as e:
                print(f"   ‚ùå Erreur de remontage: {e}")
                continue

            # V√©rification post-remontage (test simple: ls seulement, pas de lecture fichier)
            # Le test de lecture fichier est trop strict pour MEGA qui peut √™tre lent
            print(f"   üîç V√©rification post-remontage...")
            time.sleep(10)  # Attendre que rclone soit vraiment pr√™t
            health = verify_rclone_mount_healthy_simple(ip, mount_point, timeout=30)
            if health['healthy']:
                print(f"   ‚úÖ Remontage r√©ussi (temps de r√©ponse: {health['response_time']:.2f}s)")
                return True

            print(f"   ‚ùå Remontage √©chou√©: {health['error']}")

        print(f"‚ùå Impossible de restaurer le montage apr√®s {max_retries} tentatives")
        return False

    finally:
        if lock_acquired:
            global_lock.release()


def ensure_mount_healthy(ip, rclone_remote, profile, mount_point, cache_dir, log_file, phase_name):
    """
    V√©rifie que le montage rclone est fonctionnel avant une phase critique.
    Remonte automatiquement si n√©cessaire.

    Args:
        ip: 'localhost' ou IP remote
        rclone_remote: Nom du bucket S3
        profile: Profil rclone √† utiliser
        mount_point: Point de montage S3
        cache_dir: R√©pertoire de cache rclone
        log_file: Fichier de logs rclone
        phase_name: Nom de la phase pour les logs

    Returns:
        bool: True si le montage est fonctionnel, False si √©chec
    """
    print(f"   üîç V√©rification du montage S3...", end=" ", flush=True)
    # Version simple (ls avec timeout) pour √©viter les blocages sur FUSE mort
    health = verify_rclone_mount_healthy_simple(ip, mount_point)

    if health['healthy']:
        print(f"‚úÖ ({health['response_time']:.1f}s)")
        return True

    print("‚ùå")
    print(f"\n‚ö†Ô∏è  Montage S3 d√©faillant avant {phase_name}: {health['error']}")
    print("üîÑ Tentative de remontage automatique...")

    success = remount_s3_if_needed(
        ip,
        rclone_remote,
        profile=profile,
        mount_point=mount_point,
        cache_dir=cache_dir,
        log_file=log_file,
        max_retries=3
    )

    if success:
        print(f"‚úÖ Montage restaur√©, poursuite de {phase_name}")
    else:
        print(f"‚ùå Impossible de restaurer le montage pour {phase_name}")

    return success


def verify_plex_pass_active(ip, container='plex', plex_token=None, timeout=120, check_interval=10):
    """
    V√©rifie que le serveur Plex a un abonnement Plex Pass actif.

    L'analyse Sonic, intro detection, et autres features premium n√©cessitent
    un Plex Pass. Apr√®s un claim, il peut y avoir un d√©lai de propagation.

    Args:
        ip: 'localhost' ou IP remote
        container: Nom du conteneur Plex
        plex_token: Token d'authentification (requis)
        timeout: Temps max d'attente en secondes (default: 120s)
        check_interval: Intervalle entre les v√©rifications (default: 10s)

    Returns:
        dict: {
            'active': bool,           # True si Plex Pass actif
            'username': str|None,     # Nom du compte
            'subscription': str|None, # Type d'abonnement
            'features': list,         # Features premium disponibles
            'error': str|None         # Message d'erreur si √©chec
        }
    """

    if not plex_token:
        return {
            'active': False,
            'username': None,
            'subscription': None,
            'features': [],
            'error': 'Token Plex requis pour v√©rifier le Pass'
        }

    print(f"üîê V√©rification du statut Plex Pass...")

    start_time = time.time()
    last_error = None

    while time.time() - start_time < timeout:
        try:
            # Appel API pour r√©cup√©rer les infos du compte MyPlex
            api_cmd = f"curl -s 'http://localhost:32400/myplex/account' -H 'X-Plex-Token: {plex_token}'"
            result = docker_exec(ip, container, api_cmd, capture_output=True, check=False)

            if result.returncode != 0 or not result.stdout:
                last_error = "Impossible de contacter l'API Plex"
                elapsed = int(time.time() - start_time)
                print(f"   ‚è≥ Attente de la connexion √† plex.tv... ({elapsed}s)")
                time.sleep(check_interval)
                continue

            response = result.stdout

            # V√©rifier si le serveur est claim√©
            if 'authToken' not in response and 'username' not in response:
                last_error = "Serveur non claim√© ou non connect√© √† plex.tv"
                elapsed = int(time.time() - start_time)
                print(f"   ‚è≥ Attente du claim... ({elapsed}s)")
                time.sleep(check_interval)
                continue

            # Parser la r√©ponse XML
            username_match = re.search(r'username="([^"]+)"', response)
            sub_active_match = re.search(r'subscriptionActive="([^"]+)"', response)
            sub_state_match = re.search(r'subscriptionState="([^"]+)"', response)
            sub_plan_match = re.search(r'subscriptionPlan="([^"]+)"', response)

            username = username_match.group(1) if username_match else None
            sub_active = sub_active_match.group(1) if sub_active_match else "0"
            sub_state = sub_state_match.group(1) if sub_state_match else "Unknown"
            sub_plan = sub_plan_match.group(1) if sub_plan_match else None

            # Extraire les features premium
            features = re.findall(r'<Feature id="([^"]+)"', response)

            # V√©rifier le statut
            is_active = sub_active == "1"

            if is_active:
                print(f"   ‚úÖ Plex Pass ACTIF")
                print(f"      Compte: {username}")
                print(f"      Plan: {sub_plan or sub_state}")

                # Afficher les features int√©ressantes pour l'analyse
                if features:
                    sonic_features = [f for f in features if any(
                        k in f.lower() for k in ['sonic', 'loudness', 'analysis', 'music']
                    )]
                    if sonic_features:
                        print(f"      Features audio: {', '.join(sonic_features[:3])}")

                return {
                    'active': True,
                    'username': username,
                    'subscription': sub_plan or sub_state,
                    'features': features,
                    'error': None
                }
            else:
                # Pass non actif - r√©essayer (propagation en cours ?)
                elapsed = int(time.time() - start_time)
                remaining = timeout - elapsed

                if remaining > check_interval:
                    print(f"   ‚è≥ Plex Pass non d√©tect√©, nouvel essai... ({elapsed}s/{timeout}s)")
                    time.sleep(check_interval)
                else:
                    # Timeout atteint
                    print(f"   ‚ùå Plex Pass INACTIF")
                    print(f"      Compte: {username}")
                    print(f"      √âtat: {sub_state}")

                    return {
                        'active': False,
                        'username': username,
                        'subscription': sub_state,
                        'features': features,
                        'error': f"Abonnement Plex Pass non actif (√©tat: {sub_state})"
                    }

        except Exception as e:
            last_error = str(e)
            print(f"   ‚ö†Ô∏è  Erreur: {e}")
            time.sleep(check_interval)

    # Timeout global atteint
    print(f"   ‚ùå Timeout ({timeout}s) - impossible de v√©rifier")
    return {
        'active': False,
        'username': None,
        'subscription': None,
        'features': [],
        'error': last_error or f"Timeout ({timeout}s)"
    }
